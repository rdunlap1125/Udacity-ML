{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is code for looking at the problem \"Can patient healthcare records predict the likelihood of mortality in an emergency room visit?\"\n",
    "\n",
    "We start by loading the data tables for patients (https://mimic.physionet.org/mimictables/patients/) and admissions (https://mimic.physionet.org/mimictables/admissions/), followed by merging them together, adding an Age field and filtering down to ER admits.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Load libraries that will be used throughout\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from IPython.display import display \n",
    "%matplotlib inline\n",
    "\n",
    "# read patients data\n",
    "patients_data = pd.read_csv('raw_data/PATIENTS_DATA_TABLE.csv')\n",
    "print \"Patients data has {} records with {} columns.\".format(*patients_data.shape)\n",
    "\n",
    "# read admissions data\n",
    "admissions_data = pd.read_csv('raw_data/ADMISSIONS_DATA_TABLE.csv', dtype={'HADM_ID':str})\n",
    "print \"Admissions data has {} records with {} columns.\".format(*admissions_data.shape)\n",
    "\n",
    "# join the data together on SUBJECT_ID\n",
    "merged_demographic_data = pd.merge(patients_data, admissions_data, on='SUBJECT_ID')\n",
    "print \"Merged demographic data has {} records with {} columns.\".format(*merged_demographic_data.shape)\n",
    "\n",
    "# add an Age field to the dataframe\n",
    "import datetime\n",
    "def convertDatetimeField(datetime_str):\n",
    "    return datetime.datetime.strptime(datetime_str, '%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "def calcAge(birth, date):\n",
    "    age = date.year - birth.year\n",
    "    \n",
    "    if date.month < birth.month:\n",
    "        age -= 1\n",
    "    elif date.month == birth.month and date.day < birth.day:\n",
    "        age -= 1       \n",
    "        \n",
    "    if (age >=300):\n",
    "        age = 91\n",
    "        \n",
    "    return age\n",
    "\n",
    "def addAgesToData(data):\n",
    "    admits = map(convertDatetimeField, data['ADMITTIME'])\n",
    "    births = map(convertDatetimeField, data['DOB'])\n",
    "    ages = [calcAge(birth, admit) for (admit, birth) in zip(admits, births)]\n",
    "    data['AGE'] = ages\n",
    "\n",
    "addAgesToData(merged_demographic_data)\n",
    "\n",
    "# filter down to ER admits\n",
    "er_demographic_data = \\\n",
    "    merged_demographic_data[merged_demographic_data['ADMISSION_LOCATION'] == 'EMERGENCY ROOM ADMIT'].reset_index(drop=True)\n",
    "print \"ER demographic data has {} records with {} columns.\".format(*er_demographic_data.shape)\n",
    "\n",
    "print \"ER demographic data covers {} patients.\".format(len(er_demographic_data['SUBJECT_ID'].unique()))\n",
    "er_demographic_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we look at some summary statistics for the data as a whole and the ER data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printSummaryStats(data):\n",
    "    print \"In-hospital mortality rate = {}\".format(\\\n",
    "                float(data['HOSPITAL_EXPIRE_FLAG'].sum()) / data['SUBJECT_ID'].count())    \n",
    "    print \"Mean age = {}\".format(data['AGE'].mean())\n",
    "    print \"Percentage of men = {}\".format(\\\n",
    "                float(data[data['GENDER'] == 'M']['SUBJECT_ID'].count()) / data['SUBJECT_ID'].count()) \n",
    "    \n",
    "print \"MIMIC-III:\"\n",
    "printSummaryStats(merged_demographic_data)\n",
    "print \n",
    "print \"ER patients:\"\n",
    "printSummaryStats(er_demographic_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This was some code used to look at the raw mortality rate data; the bar graphs are another way of looking at this, but the function is also used elsewhere in the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def calcMortalityRateGroupedByFeature(data, feature):\n",
    "    count = data.groupby(feature).count()['HADM_ID']\n",
    "    expires = data.groupby(feature).sum()['HOSPITAL_EXPIRE_FLAG']\n",
    "    \n",
    "    print\n",
    "    print \"Mortality rates by {}: {} distinct items\".format(feature, len(count))\n",
    "    for key in count.keys():\n",
    "        print key, float(expires[key]) / count[key], count[key]\n",
    "    \n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'AGE')\n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'GENDER')\n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'INSURANCE')\n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'MARITAL_STATUS')\n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'ETHNICITY')\n",
    "#calcMortalityRateGroupedByFeature(er_demographic_data, 'RELIGION')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a helper method for sifting out the values whose frequency in the data set is above a given threshold.  The threshold can either be a percentage between 0.0 and 1.0, or an absolute number of values > 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def calculateValuesAboveThresholdForKey(data, key, threshold):\n",
    "    value_counts = data[key].value_counts()\n",
    "    total = np.sum(value_counts)\n",
    "    if (threshold <= 1.0):  #treat it as a percentage\n",
    "        values = value_counts[value_counts > threshold * total].keys()\n",
    "    else: #treat it as a minimum number of values\n",
    "        values = value_counts[value_counts > threshold].keys()\n",
    "    return values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This a method for visualizing frequencies across a given key dimension -- I took the survival_stats function from P0 as a\n",
    "starting point and modified it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# keyValues allows a specific set of keys to be considered -- the default behavior is to use all keys.  This only works\n",
    "# with categorical data\n",
    "#\n",
    "# thresholdPct allows specification of threshold for how frequently a key must appear to be graphed as a percentage of all rows \n",
    "# -- this again only works with categorical data\n",
    "#\n",
    "# figSize allows an alternate value to be passed in if the default figsize is crunching the y-axis too much.  This was an issue\n",
    "# for ETHNICITY\n",
    "#\n",
    "# if both key values and thresholdPct are specified, keyValues overrides thresholdPct\n",
    "def visualizeFrequencyData(data, key, keyValues = [], thresholdPct = 0.0, figsize = (8, 6)):\n",
    "    \n",
    "    # Check that the key exists\n",
    "    if key not in data.columns.values :\n",
    "        print \"'{}' is not a feature of the data.\".format(dimension)\n",
    "        return False    \n",
    "   \n",
    "    all_data = data.copy()\n",
    "    \n",
    "    # Create a data frame for the \n",
    "    all_data = all_data[[key]]\n",
    "    \n",
    "    # Create plotting figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # 'Numerical' features\n",
    "    if(key == 'AGE'):\n",
    "        \n",
    "        # Remove NaN values from Age data\n",
    "        chart_data = all_data[~np.isnan(all_data[key])]\n",
    "        \n",
    "        # Divide the range of data into bins and count survival rates\n",
    "        min_value = chart_data[key].min()\n",
    "        max_value = chart_data[key].max()\n",
    "        value_range = max_value - min_value\n",
    "\n",
    "        bins = np.arange(10, chart_data['AGE'].max() + 10, 10)\n",
    "        \n",
    "        # Overlay each bin's survival rates\n",
    "        binned_total, bins = np.histogram(chart_data, bins)\n",
    "       \n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.8\n",
    "       \n",
    "        tick_labels = []\n",
    "        for i in np.arange(len(binned_total)):\n",
    "            plt.barh(i - bar_width/2.0, binned_total[i], height = bar_width, color = 'r')\n",
    "            tick_labels.append(\"{} - {}\".format(bins[i], bins[i+1]))\n",
    "        \n",
    "        # Add legend to plot\n",
    "        plt.yticks(np.arange(len(tick_labels)), tick_labels)        \n",
    "    \n",
    "    # 'Categorical' features\n",
    "    else:\n",
    "        if len(keyValues) > 0:\n",
    "            values = keyValues\n",
    "        else:\n",
    "            # This was put in to use thresholdPct, but it has the nice side effect of also sorting the graph by frequency, \n",
    "            # so I'm using it even when thresholdPct = 0.0\n",
    "            values = calculateValuesAboveThresholdForKey(all_data, key, thresholdPct)\n",
    "\n",
    "        # Create DataFrame containing categories and count of each\n",
    "        frame = pd.DataFrame(index = np.arange(len(values)), columns=(key,'Total'))\n",
    "        for i, value in enumerate(reversed(values)):\n",
    "            frame.loc[i] = [value, \\\n",
    "                   len(all_data[all_data[key] == value])]\n",
    "\n",
    "        # Set the width of each bar\n",
    "        bar_width = .8\n",
    "\n",
    "        # Display each category's survival rates\n",
    "        for i in np.arange(len(frame)):\n",
    "            plt.barh(i - bar_width/2.0, frame.loc[i]['Total'], height = bar_width, color = 'r')\n",
    "\n",
    "        plt.yticks(np.arange(len(frame)), reversed(values))\n",
    "\n",
    "    # Common attributes for plot formatting\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel(\"Number of Visits\")\n",
    "    title = \"Visit Frequency Statistics With \\'{}\\' Feature\".format(key)\n",
    "    if sum(pd.isnull(all_data[key])) > 0:\n",
    "        nan_outcomes = all_data[pd.isnull(all_data[key])]\n",
    "        title += \"\\nVisits with missing values: {}\".format(len(nan_outcomes))  \n",
    "    plt.title(title)\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method above to visualize the frequency stats for demographic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "visualizeFrequencyData(er_demographic_data, 'AGE')  \n",
    "visualizeFrequencyData(er_demographic_data, 'GENDER')\n",
    "visualizeFrequencyData(er_demographic_data, 'INSURANCE')\n",
    "visualizeFrequencyData(er_demographic_data, 'MARITAL_STATUS')\n",
    "visualizeFrequencyData(er_demographic_data, 'ETHNICITY', figsize=(8, 10))\n",
    "visualizeFrequencyData(er_demographic_data, 'RELIGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the results above, do some data cleaning; see the report for a more in depth discussion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Merge infrequent values and 'None' values into 'OTHER'\n",
    "for feature in ['MARITAL_STATUS', 'ETHNICITY', 'RELIGION']:\n",
    "    topValues = calculateValuesAboveThresholdForKey(er_demographic_data, feature, .01)\n",
    "    er_demographic_data.loc[~er_demographic_data[feature].isin(topValues), feature] = 'OTHER'\n",
    "    \n",
    "# Set fields corresponding to concepts of 'unspecified' or 'other' to None for consistency\n",
    "er_demographic_data.loc[er_demographic_data['RELIGION'].isin(['UNOBTAINABLE', 'NOT SPECIFIED']), 'RELIGION'] = 'OTHER'\n",
    "er_demographic_data.loc[er_demographic_data['ETHNICITY'].isin(['UNKNOWN/NOT SPECIFIED']), 'ETHNICITY'] = 'OTHER'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a similar method to the above for looking at mortality rates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "# keyValues allows a specific set of keys to be considered -- the default behavior is to use all keys.  This only works\n",
    "# with categorical data\n",
    "#\n",
    "# thresholdPct allows specification of threshold for how frequently a key must appear to be graphed as a percentage of all rows \n",
    "# -- this again only works with categorical data\n",
    "#\n",
    "# figSize allows an alternate value to be passed in if the default figsize is crunching the y-axis too much.  This was an issue\n",
    "# for ETHNICITY before cleaning\n",
    "#\n",
    "# if both key values and thresholdPct are specified, keyValues overrides thresholdPct\n",
    "#\n",
    "def visualizeExpiryData(data, key, keyValues = [], thresholdPct = 0.0, figsize=(8, 6)):\n",
    "    \n",
    "    # Check that the key exists\n",
    "    if key not in data.columns.values :\n",
    "        print \"'{}' is not a feature of the data.\".format(dimension)\n",
    "        return False    \n",
    "   \n",
    "    all_data = data.copy()\n",
    "    \n",
    "    all_data = all_data[[key, 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    \n",
    "    # Create plotting figure\n",
    "    plt.figure(figsize=figsize)\n",
    "\n",
    "    # 'Numerical' features\n",
    "    if(key == 'AGE'):\n",
    "        \n",
    "        # Remove NaN values from Age data,if any\n",
    "        chart_data = all_data[~np.isnan(all_data[key])]\n",
    "        \n",
    "        # Divide the range of data into bins and count survival rates\n",
    "        min_value = chart_data[key].min()\n",
    "        max_value = chart_data[key].max()\n",
    "        value_range = max_value - min_value\n",
    "\n",
    "        bins = np.arange(10, chart_data['AGE'].max() + 10, 10)\n",
    "        \n",
    "        # Overlay each bin's survival rates\n",
    "        nonsurv_vals = chart_data[chart_data['HOSPITAL_EXPIRE_FLAG'] == 1][key].reset_index(drop = True)\n",
    "        \n",
    "        binned_nonsurv_vals, bins = np.histogram(nonsurv_vals, bins)\n",
    "        binned_total, bins = np.histogram(chart_data, bins)\n",
    "       \n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.8\n",
    "       \n",
    "        tick_labels = []\n",
    "        for i in np.arange(len(binned_total)):\n",
    "            plt.barh(i - bar_width/2.0, 1.0*binned_nonsurv_vals[i]/binned_total[i], height = bar_width, color = 'r')\n",
    "            tick_labels.append(\"{} - {}\".format(bins[i], bins[i+1]))\n",
    "        \n",
    "        # Add legend to plot\n",
    "        plt.yticks(np.arange(len(tick_labels)), tick_labels)        \n",
    "    \n",
    "    # 'Categorical' features\n",
    "    else:\n",
    "        if len(keyValues) > 0:\n",
    "            values = keyValues\n",
    "        else:\n",
    "            # This was put in to use thresholdPct, but it has the nice side effect of also sorting the graph by frequency, \n",
    "            # so I'm using it even when thresholdPct = 0.0\n",
    "            values = calculateValuesAboveThresholdForKey(all_data, key, thresholdPct)\n",
    "\n",
    "        # Create DataFrame containing categories and count of each\n",
    "        frame = pd.DataFrame(index = np.arange(len(values)), columns=(key,'Died','Total'))\n",
    "        for i, value in enumerate(reversed(values)):\n",
    "            frame.loc[i] = [value, \\\n",
    "                   len(all_data[(all_data['HOSPITAL_EXPIRE_FLAG'] == 1) & (all_data[key] == value)]), \\\n",
    "                   len(all_data[all_data[key] == value])]\n",
    "\n",
    "        # Set the width of each bar\n",
    "        bar_width = 0.8\n",
    "\n",
    "        # Display each category's survival rates\n",
    "        for i in np.arange(len(frame)):\n",
    "            plt.barh(i - bar_width/2.0, 1.0 * frame.loc[i]['Died'] / frame.loc[i]['Total'], height = bar_width, color = 'r')\n",
    "\n",
    "        plt.yticks(np.arange(len(frame)), reversed(values))\n",
    "\n",
    "    # Common attributes for plot formatting\n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel(\"Mortality Rate\")\n",
    "    title = \"Visit Mortality Rate Across \\'{}\\' Feature\".format(key)\n",
    "    if sum(pd.isnull(all_data[key])) > 0:\n",
    "        nan_outcomes = all_data[pd.isnull(all_data[key])]['HOSPITAL_EXPIRE_FLAG']\n",
    "        title += \"\\nVisits with missing values: {} (Mortality rate = {:0.3f})\".format( \\\n",
    "                      len(nan_outcomes), 1.0*sum(nan_outcomes == 1)/sum(nan_outcomes == 0))  \n",
    "    plt.title(title)\n",
    "    plt.show()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the method above to visualize the mortality rates across various demographic features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "visualizeExpiryData(er_demographic_data, 'AGE')  \n",
    "visualizeExpiryData(er_demographic_data, 'GENDER')\n",
    "visualizeExpiryData(er_demographic_data, 'INSURANCE')\n",
    "visualizeExpiryData(er_demographic_data, 'MARITAL_STATUS')\n",
    "visualizeExpiryData(er_demographic_data, 'ETHNICITY')\n",
    "visualizeExpiryData(er_demographic_data, 'RELIGION')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a method for looking at how two features correlate -- for the second feature, it plots one line graph\n",
    "for each value of the feature showing the percentage of the population that has that value for each value of the first\n",
    "dimension.  The method is then applied to Age vs. the other demographic dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def compareFeatures(data, feature1, feature2, thresholdPct = 0.0):\n",
    "    comparison_data = data[[feature1, feature2]]\n",
    "    values = calculateValuesAboveThresholdForKey(comparison_data, feature2, thresholdPct)\n",
    "    comparison_data = comparison_data[comparison_data[feature2].isin(values)]\n",
    "    comparison_data = pd.get_dummies(comparison_data, columns = [feature2])\n",
    "    comparison_data = comparison_data.groupby(feature1).sum()\n",
    "    comparison_data = comparison_data.div(comparison_data.sum(axis=1), axis=0)\n",
    "    comparison_data.plot(figsize = (8, 6))\n",
    "\n",
    "compareFeatures(er_demographic_data, 'AGE', 'INSURANCE')\n",
    "compareFeatures(er_demographic_data, 'AGE', 'MARITAL_STATUS')\n",
    "compareFeatures(er_demographic_data, 'AGE', 'ETHNICITY')\n",
    "compareFeatures(er_demographic_data, 'AGE', 'RELIGION')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This method calculates a pivot table of mortality rates.  One dimension is age sorted into bins, starting from 10-20 and going up through 90-100, and the second dimension is an argument to the function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Calculates a pivot of mortality rate by Age and one other input feature\n",
    "def calcMortalityRatePivot(input_data, key):\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    data = data[['AGE', key, 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    \n",
    "    min_value = data['AGE'].min()\n",
    "    max_value = data['AGE'].max()\n",
    "    value_range = max_value - min_value\n",
    "\n",
    "    bins = np.arange(10, data['AGE'].max() + 10, 10)        \n",
    "    data['AGE_BIN'] = np.digitize(data['AGE'], bins)\n",
    "    \n",
    "    key_values = data[key].unique()\n",
    "    num_key_values = len(key_values)\n",
    "    \n",
    "    frame = pd.DataFrame(index = np.arange(len(bins)-1 * len(key_values)), \n",
    "                         columns=('AGE_BIN', key,'Died','Total'))    \n",
    "    for i in (range(1, len(bins))):\n",
    "        for j, value in enumerate(key_values):\n",
    "            frame.loc[(i-1)*num_key_values + j] = \\\n",
    "                        [i, value, \\\n",
    "                         len(data[(data['HOSPITAL_EXPIRE_FLAG'] == 1) \n",
    "                                  & (data['AGE_BIN'] == i) & (data[key] == value)]), \\\n",
    "                         len(data[(data['AGE_BIN'] == i) & (data[key] == value)])]   \n",
    "    frame['Mortality Rate'] = frame['Died'].astype(\"float64\") / frame['Total'].astype(\"float64\")\n",
    "\n",
    "    display(frame.pivot(index='AGE_BIN', columns=key, values='Mortality Rate'))\n",
    "\n",
    "calcMortalityRatePivot(er_demographic_data, 'INSURANCE')\n",
    "calcMortalityRatePivot(er_demographic_data, 'RELIGION')\n",
    "calcMortalityRatePivot(er_demographic_data, 'ETHNICITY')\n",
    "calcMortalityRatePivot(er_demographic_data, 'MARITAL_STATUS')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Trim the data down to only columns needed for the analysis; HADM_ID is left in place to facilitate merging with diagnosis data later.  Then, replace the demographic fields with binary valued fields, one for each value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Cut the columns down to only include fields that we'll use in the analysis\n",
    "er_demo_analysis_data = \\\n",
    "    er_demographic_data[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG', 'AGE', \\\n",
    "                         'INSURANCE', 'MARITAL_STATUS', 'ETHNICITY', 'RELIGION', 'GENDER']]  \n",
    "er_demo_analysis_data = \\\n",
    "    pd.get_dummies(er_demo_analysis_data, columns=['INSURANCE', 'MARITAL_STATUS', 'ETHNICITY', 'RELIGION', 'GENDER'])\n",
    "    \n",
    "print er_demo_analysis_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now,load the diagnoses data (https://mimic.physionet.org/mimictables/diagnoses_icd/) and the diagnoses dictionary (https://mimic.physionet.org/mimictables/d_icd_diagnoses/).  Filter down to rows associated with previously identified ER visits, and do some minor cleanup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#read diagnoses data\n",
    "diagnosis_data = pd.read_csv('raw_data/DIAGNOSES_ICD_DATA_TABLE.csv', dtype={'HADM_ID':str})\n",
    "\n",
    "print \"Diagnosis data has {} records with {} columns.\".format(*diagnosis_data.shape)\n",
    "\n",
    "#Read in the diagnosis dictionary for later use\n",
    "diagnosis_dictionary = pd.read_csv('raw_data/D_ICD_DIAGNOSES_DATA_TABLE.csv')\n",
    "\n",
    "#filter down to the ER admissions \n",
    "er_admits = er_demographic_data['HADM_ID']\n",
    "er_diagnosis_data = diagnosis_data[diagnosis_data['HADM_ID'].isin(er_admits)]\n",
    "\n",
    "#Some minor data cleanup here -- drop rows where there is no ICD9_CODE. \n",
    "print \"There are {} records with no ICD9_CODE\".format(len(er_diagnosis_data[er_diagnosis_data['ICD9_CODE'].isnull()]))\n",
    "er_diagnosis_data = er_diagnosis_data[~er_diagnosis_data['ICD9_CODE'].isnull()].reset_index(drop=True)\n",
    "\n",
    "#Drop all but the HADM_ID and ICD9_CODE\n",
    "er_diagnosis_data = er_diagnosis_data[['HADM_ID', 'ICD9_CODE']]\n",
    "\n",
    "print \"Diagnosis data for ER admits has {} records with {} columns.\".format(*er_diagnosis_data.shape)\n",
    "\n",
    "print er_diagnosis_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do some initial data summaries of both the diagnoses data as a whole and the ER diagnoses set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def printDiagnosisDataSummary(input_data):\n",
    "    data = input_data.copy()\n",
    "    \n",
    "    #Add in the data about in-hospital deaths\n",
    "    data = pd.merge(data, merged_demographic_data[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG']], on='HADM_ID')\n",
    "    \n",
    "    diagnosis_count = data['ICD9_CODE'].value_counts().reset_index()\n",
    "    diagnosis_count.columns=['ICD9_CODE', 'Visits']\n",
    "    diagnosis_deaths = data[data['HOSPITAL_EXPIRE_FLAG'] == 1]['ICD9_CODE'].value_counts().reset_index()\n",
    "    diagnosis_deaths.columns=['ICD9_CODE', 'Deaths']\n",
    "\n",
    "    print \"Diagnosis data has {} unique diagnosis codes.\".format(len(diagnosis_count))\n",
    "    print \"Diagnosis data has {} unique diagnosis codes associated with in-hospital deaths.\".format(len(diagnosis_deaths))\n",
    "\n",
    "    diagnosis_mortality_data = pd.merge(diagnosis_count, diagnosis_deaths, on='ICD9_CODE', how='left')\n",
    "    diagnosis_mortality_data['Deaths'].fillna(value=0, inplace=True)\n",
    "    diagnosis_mortality_data['Mortality Rate'] = diagnosis_mortality_data['Deaths']/diagnosis_mortality_data['Visits']\n",
    "    diagnosis_mortality_data = pd.merge(diagnosis_mortality_data, diagnosis_dictionary, on='ICD9_CODE').drop('ROW_ID', axis=1)\n",
    "\n",
    "    display(diagnosis_mortality_data[0:10].drop('SHORT_TITLE', axis=1))\n",
    "    display(diagnosis_mortality_data.sort_values(by='Mortality Rate', ascending=False)[0:10].drop('SHORT_TITLE', axis=1))\n",
    "\n",
    "print \"All data\"\n",
    "printDiagnosisDataSummary(diagnosis_data)\n",
    "print \"ER data\"\n",
    "printDiagnosisDataSummary(er_diagnosis_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Deal with some outlier issues -- there are three approaches.\n",
    "* We can look at only codes with sequence number = 1 -- the primary codes for the visit.  This was not used in the final analysis, as discussed in the report.\n",
    "* We can clean out codes that occur infrequently\n",
    "* We can group the codes together as done in the MIMIC III paper\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#For the final analysis, SEQ_NUM was not used and was dropped from the data set\n",
    "#printDiagnosisDataSummary(er_diagnosis_data[er_diagnosis_data['SEQ_NUM']==1])\n",
    "\n",
    "#The 1% standard cuts us down to nine diagnoses, which is too few -- so we use a raw number instead\n",
    "frequentDiagnoses = calculateValuesAboveThresholdForKey(er_diagnosis_data, 'ICD9_CODE', 100)\n",
    "er_frequent_diagnosis_data = er_diagnosis_data[er_diagnosis_data['ICD9_CODE'].isin(frequentDiagnoses)]\n",
    "printDiagnosisDataSummary(er_frequent_diagnosis_data)\n",
    "\n",
    "#Calc ICD-9 grouping as in the MIMIC-III paper.  Rather than write long text summaries, just letter then from A through J.\n",
    "def calcDiagnosisGroup(code):\n",
    "    #print code, type(code)\n",
    "    codePrefix = code[:3]\n",
    "    if \"001\" <= codePrefix <= \"139\":\n",
    "        return \"A\"\n",
    "    elif \"140\" <= codePrefix <= \"239\":\n",
    "        return \"B\"    \n",
    "    elif \"240\" <= codePrefix <= \"279\":\n",
    "        return \"C\" \n",
    "    elif \"390\" <= codePrefix <= \"459\":\n",
    "        return \"D\" \n",
    "    elif \"460\" <= codePrefix <= \"519\":\n",
    "        return \"E\" \n",
    "    elif \"520\" <= codePrefix <= \"579\":\n",
    "        return \"F\" \n",
    "    elif \"580\" <= codePrefix <= \"629\":\n",
    "        return \"G\" \n",
    "    elif \"800\" <= codePrefix <= \"959\":\n",
    "        return \"H\" \n",
    "    elif \"960\" <= codePrefix <= \"979\":\n",
    "        return \"I\" \n",
    "    else:\n",
    "        return \"J\"\n",
    "\n",
    "codeGroups = [calcDiagnosisGroup(code) for code in er_diagnosis_data['ICD9_CODE']]\n",
    "er_diagnosis_data['DIAGNOSIS_GROUP'] = codeGroups\n",
    "er_grouped_diagnosis_data = er_diagnosis_data.drop('ICD9_CODE', axis=1)\n",
    "calcMortalityRateGroupedByFeature( \\\n",
    "    pd.merge(er_grouped_diagnosis_data, merged_demographic_data[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG']], on='HADM_ID'),\\\n",
    "    'DIAGNOSIS_GROUP')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Replace the DIAGNOSIS_GROUP and ICD9_CODE fields with binary valued dummy fields, as with the demographic fields.  Then group all the information associated with each distinct visit on the same row and merge the diagnosis data back together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Convert to binary variables, and then collapse values with the same HADM_IDs together\n",
    "er_grouped_diagnosis_data = pd.get_dummies(er_grouped_diagnosis_data, columns=['DIAGNOSIS_GROUP'])\n",
    "er_grouped_diagnosis_data = er_grouped_diagnosis_data.groupby(er_grouped_diagnosis_data['HADM_ID']).sum().reset_index()\n",
    "print er_grouped_diagnosis_data.loc[0]\n",
    "\n",
    "er_frequent_diagnosis_data = pd.get_dummies(er_frequent_diagnosis_data, columns=['ICD9_CODE'])\n",
    "er_frequent_diagnosis_data = er_frequent_diagnosis_data.groupby(er_frequent_diagnosis_data['HADM_ID']).sum().reset_index()\n",
    "print er_frequent_diagnosis_data.loc[0]\n",
    "\n",
    "er_diagnosis_data = pd.merge(er_grouped_diagnosis_data, er_frequent_diagnosis_data, on='HADM_ID', how='left')\n",
    "for key in er_diagnosis_data.keys():\n",
    "    er_diagnosis_data[key].fillna(value=0, inplace=True)\n",
    "\n",
    "print er_diagnosis_data.loc[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Set up some reusable machinery for training and testing learners."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.metrics import accuracy_score, f1_score, precision_recall_fscore_support\n",
    "\n",
    "#Make sure to use the same split here and when tuning later\n",
    "def splitData(features, labels):\n",
    "    return train_test_split(features, labels, test_size=0.2, random_state=42)\n",
    "\n",
    "def testClassifier(clf, features, labels):\n",
    "    features_train, features_test, labels_train, labels_test = splitData(features, labels)\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    print \"Classifier {}:\".format(type(clf).__name__)\n",
    "    print \"F1 score = {}\".format(f1_score(labels_test, pred))\n",
    "    print \"Total deaths in test set = {}; total predicted = {}\".format(sum(labels_test), sum(pred))\n",
    "    print precision_recall_fscore_support(labels_test, pred, average='binary')\n",
    "    print\n",
    "    \n",
    "\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "def testMultipleClassifiers(data, labels):\n",
    "    testClassifier(GaussianNB(), data, labels)\n",
    "    testClassifier(DecisionTreeClassifier(random_state=38), data, labels)\n",
    "    testClassifier(AdaBoostClassifier(random_state=38), data, labels)\n",
    "    testClassifier(KNeighborsClassifier(), data, labels)\n",
    "    testClassifier(RandomForestClassifier(random_state=38), data, labels)\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some resuable machinery for generating feature and label data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def buildDemographicsOnlyData():\n",
    "    labels = er_demo_analysis_data['HOSPITAL_EXPIRE_FLAG']\n",
    "    er_analysis_data = er_demo_analysis_data.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "    \n",
    "    return (er_analysis_data, labels)\n",
    "\n",
    "def buildDiagnosesOnlyData():\n",
    "    er_results = er_demographic_data[['HADM_ID', 'HOSPITAL_EXPIRE_FLAG']]\n",
    "    er_analysis_data = pd.merge(er_diagnosis_data, er_results, on='HADM_ID')\n",
    "\n",
    "    labels = er_analysis_data['HOSPITAL_EXPIRE_FLAG']\n",
    "    er_analysis_data = er_analysis_data.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "    \n",
    "    return (er_analysis_data, labels)    \n",
    "\n",
    "def buildMergedData():\n",
    "    er_combined_data = pd.merge(er_demo_analysis_data, er_diagnosis_data, on='HADM_ID') \n",
    "\n",
    "    labels = er_combined_data['HOSPITAL_EXPIRE_FLAG']\n",
    "    er_analysis_data = er_combined_data.drop('HOSPITAL_EXPIRE_FLAG', axis=1)\n",
    "    \n",
    "    return (er_analysis_data, labels)        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training using only the demographic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildDemographicsOnlyData()\n",
    "\n",
    "testMultipleClassifiers(er_analysis_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform training using only the diagnostic data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildDiagnosesOnlyData()\n",
    "\n",
    "testMultipleClassifiers(er_analysis_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perform training using the combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildMergedData()\n",
    "\n",
    "testMultipleClassifiers(er_analysis_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Set up some reusable machinery for the tuning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.metrics import make_scorer\n",
    "\n",
    "def tuneClassifier(clf, parameters, data, labels):\n",
    "    features_train, features_test, labels_train, labels_test = splitData(er_analysis_data, labels)\n",
    "    \n",
    "    f1_scorer = make_scorer(f1_score)\n",
    "    grid_obj = GridSearchCV(clf, parameters, f1_scorer)\n",
    "\n",
    "    grid_obj = grid_obj.fit(features_train, labels_train)\n",
    "\n",
    "    clf = grid_obj.best_estimator_\n",
    "\n",
    "    pred = clf.predict(features_test)\n",
    "    print \"Classifier {}:\".format(type(clf).__name__)\n",
    "    print \"Parameters: {}\".format(clf.get_params())\n",
    "    print \"CV F1 score = {}\".format(grid_obj.best_score_)\n",
    "    print \"Total deaths in test set = {}; total predicted = {}\".format(sum(labels_test), sum(pred))\n",
    "    print precision_recall_fscore_support(labels_test, pred, average='binary')\n",
    "    print    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Tune the AdaBoost classifier on combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildMergedData()\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=38)\n",
    "\n",
    "#parameters = {'n_estimators':np.arange(650, 751, 10), \\\n",
    "#              'learning_rate':np.linspace(.8, 1.0, 5)}\n",
    "\n",
    "parameters = {'n_estimators':[730], \\\n",
    "              'learning_rate':[0.9]}\n",
    "\n",
    "tuneClassifier(clf, parameters, er_analysis_data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the Decision Tree classifier on combined data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildMergedData()\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=38)\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], \\\n",
    "              'min_samples_split':np.arange(17, 19, 1), \\\n",
    "              'max_features': ['sqrt', 'log2', None]}\n",
    "\n",
    "tuneClassifier(clf, parameters, er_analysis_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the AdaBoost classifier on diagnoses data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildDiagnosesOnlyData()\n",
    "\n",
    "clf = AdaBoostClassifier(random_state=38)\n",
    "\n",
    "parameters = {'n_estimators':np.arange(200, 301, 10), \\\n",
    "              'learning_rate':np.linspace(.8, 1.0, 5)}\n",
    "\n",
    "tuneClassifier(clf, parameters, er_analysis_data, labels)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tune the Decision Tree classifier on diagnoses data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildDiagnosesOnlyData()\n",
    "\n",
    "clf = DecisionTreeClassifier(random_state=38)\n",
    "\n",
    "parameters = {'criterion': ['gini', 'entropy'], \\\n",
    "              'min_samples_split':np.arange(38, 43, 1), \\\n",
    "              'max_features': ['sqrt', 'log2', None]}\n",
    "\n",
    "tuneClassifier(clf, parameters, er_analysis_data, labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Look at the robustness of the final AdaBoost classifier by running over different splits -- no random_state settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildMergedData()\n",
    "f1_scores = []\n",
    "for i in range(0, 100):\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "            train_test_split(er_analysis_data, labels, test_size=0.2)\n",
    "    clf = AdaBoostClassifier(n_estimators=730, learning_rate=0.9)\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    f1_scores.append(f1_score(labels_test, pred))\n",
    "    \n",
    "pd.DataFrame(f1_scores).describe()  \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do the same analysis, but look at the best result for the diagnosis data alone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "er_analysis_data, labels = buildDiagnosesOnlyData()\n",
    "f1_scores = []\n",
    "for i in range(0, 100):\n",
    "    features_train, features_test, labels_train, labels_test = \\\n",
    "            train_test_split(er_analysis_data, labels, test_size=0.2)\n",
    "    clf = AdaBoostClassifier(n_estimators=220, learning_rate=1.0)\n",
    "    clf.fit(features_train, labels_train)\n",
    "    pred = clf.predict(features_test)\n",
    "    f1_scores.append(f1_score(labels_test, pred))\n",
    "    \n",
    "pd.DataFrame(f1_scores).describe()  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the F1 scores crosscut by individual values of dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotCrosscutScores(key, clf, features, labels):\n",
    "    \n",
    "    if key=='AGE':\n",
    "        data = features.copy()\n",
    "        min_value = data['AGE'].min()\n",
    "        max_value = data['AGE'].max()\n",
    "        value_range = max_value - min_value\n",
    "\n",
    "        bins = np.arange(10, data['AGE'].max() + 10, 10)        \n",
    "        data['AGE_BIN'] = np.digitize(data['AGE'], bins)\n",
    "        \n",
    "        df = pd.DataFrame(index = np.arange(0, len(bins) - 1), columns=['AGE','F1 Score'])       \n",
    "        for i in range(1, len(bins)):\n",
    "            subset_index = [index for index in data.index if data.loc[index, 'AGE_BIN'] == i]                     \n",
    "            pred = clf.predict(data.loc[subset_index].drop(['AGE_BIN'], axis=1))\n",
    "            score = f1_score(labels.loc[subset_index], pred)\n",
    "            df.loc[i-1] = ['{} - {}'.format(i*10,(i+1)*10), score]     \n",
    "    else:\n",
    "        if key=='ICD9_CODE':\n",
    "            values = ['V667', '2866', '4275', '3481', '5724', '570', '41091', '78551', '1972', '42741']  #our top 10 codes from earlier\n",
    "        else:\n",
    "            #Since features have been converted to dummy variables, have to go back to demographic data for a values list            \n",
    "            values = er_demographic_data[key].unique()\n",
    "        df = pd.DataFrame(index = np.arange(0, len(values)), columns=[key,'F1 Score'])        \n",
    "        for (i, value) in enumerate(values):\n",
    "            subset_index = [index for index in features.index if features.loc[index, key + '_' + value] == 1]\n",
    "            pred = clf.predict(features.loc[subset_index])\n",
    "            score = f1_score(labels.loc[subset_index], pred)\n",
    "            df.loc[i] = [value, score]\n",
    "          \n",
    "    plt.figure(figsize=(8,6))\n",
    "    # Set the width of each bar\n",
    "    bar_width = 0.8\n",
    "\n",
    "    for i in np.arange(len(df.index)):\n",
    "        plt.barh(i - bar_width/2.0, df.loc[i, 'F1 Score'], height = bar_width, color = 'r')\n",
    "\n",
    "    plt.yticks(np.arange(len(df[key].values)), df[key].values)    \n",
    "    plt.ylabel(key)\n",
    "    plt.xlabel(\"F1 Score\")\n",
    "    plt.title(\"F1 Score by Value For Feature {}\".format(key))\n",
    "    plt.show()        \n",
    "\n",
    "er_analysis_data, labels = buildMergedData()\n",
    "features_train, features_test, labels_train, labels_test = splitData(er_analysis_data, labels)\n",
    "clf = AdaBoostClassifier(n_estimators=730, learning_rate=0.9, random_state=38)\n",
    "clf.fit(features_train, labels_train)\n",
    "\n",
    "plotCrosscutScores('AGE', clf, features_test, labels_test)\n",
    "plotCrosscutScores('GENDER', clf, features_test, labels_test)\n",
    "plotCrosscutScores('INSURANCE', clf, features_test, labels_test)\n",
    "plotCrosscutScores('ETHNICITY', clf, features_test, labels_test)\n",
    "plotCrosscutScores('RELIGION', clf, features_test, labels_test)\n",
    "plotCrosscutScores('ICD9_CODE', clf, features_test, labels_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
